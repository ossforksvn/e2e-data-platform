{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173a47df-b59e-400e-bd18-80cfe8ccdf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "437ab558-e0ba-4261-9587-130b89fc3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conf = SparkConf() \\\n",
    "    .set(\"spark.driver.cores\", \"1\") \\\n",
    "    .set(\"spark.driver.memory\", \"512m\") \\\n",
    "    .set(\"spark.executor.cores\", \"1\") \\\n",
    "    .set(\"spark.executor.memory\", \"512m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ee8028-b86f-4863-810e-e9fd1f7f81e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/11 08:13:37 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 172.22.0.13 instead (on interface eth0)\n",
      "25/07/11 08:13:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8a9696a3-7665-4eba-bf10-aa5a35021991;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.6 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.367 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.1.3.Final in central\n",
      "\tfound org.postgresql#postgresql;42.2.24 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.4.0/delta-core_2.12-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.4.0!delta-core_2.12.jar (4594ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.6/hadoop-aws-3.3.6.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.6!hadoop-aws.jar (952ms)\n",
      "downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.24/postgresql-42.2.24.jar ...\n",
      "\t[SUCCESSFUL ] org.postgresql#postgresql;42.2.24!postgresql.jar (936ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.4.0/delta-storage-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.4.0!delta-storage.jar (432ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (536ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.367/aws-java-sdk-bundle-1.12.367.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.367!aws-java-sdk-bundle.jar (229897ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.1.3.Final/wildfly-openssl-1.1.3.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.1.3.Final!wildfly-openssl.jar (638ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.5.0!checker-qual.jar (460ms)\n",
      ":: resolution report :: resolve 15359ms :: artifacts dl 238456ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.367 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.2.24 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.1.3.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 by [com.amazonaws#aws-java-sdk-bundle;1.12.367] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   8   |   8   |   1   ||   8   |   8   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8a9696a3-7665-4eba-bf10-aa5a35021991\n",
      "\tconfs: [default]\n",
      "\t8 artifacts copied, 0 already retrieved (310227kB/248ms)\n",
      "25/07/11 08:17:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Creating Delta Schema\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b023b689-c247-476c-a68a-26432e04e68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='bronze', catalog='spark_catalog', description='', locationUri='s3a://delta/bronze'), Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='s3://delta/'), Database(name='gold', catalog='spark_catalog', description='', locationUri='s3a://delta/gold'), Database(name='raw', catalog='spark_catalog', description='', locationUri='s3a://datalake/iceberg/raw'), Database(name='silver', catalog='spark_catalog', description='', locationUri='s3a://delta/silver')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# spark\n",
    "\n",
    "# for item in spark.sparkContext.getConf().getAll():\n",
    "#     print(item)\n",
    "\n",
    "print(spark.catalog.listDatabases())\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "#     SHOW TABLES IN bronze\n",
    "# \"\"\")\n",
    "\n",
    "# spark.sql(\"DESCRIBE DATABASE EXTENDED bronze\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27639015-d85c-4560-9c71-935e935b737c",
   "metadata": {},
   "source": [
    "### 1. create_schema.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "277961e3-d373-413f-8eee-ff9d5dc41723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE IF EXISTS bronze CASCADE\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS bronze LOCATION 's3a://delta/bronze'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "017a2096-1ac9-4b34-9ec2-fca19cffa38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE IF EXISTS silver2 CASCADE\")\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "#     CREATE DATABASE IF NOT EXISTS silver2 LOCATION 's3a://delta/silver2'\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52241b78-c03e-413c-9099-cea55ea77c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
