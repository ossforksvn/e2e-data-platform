#-------------------------------
# Environment Variables for Data platform
#-------------------------------

# ------------------------------
# Common Variables
# ------------------------------

# Datalake s3 bucket name
DATALAKE_BUCKET=delta
WAREHOUSE_DIR=s3://$DATALAKE_BUCKET/

# ------------------------------
# Minio configuration
# This Minio configuration is used for storage layer
# ------------------------------

# Access key and secret key for Minio, like AWS S3
# Use access key and secret key to login to Minio and to access Minio storage from other services like Spark, Hive, etc.
MINIO_ACCESS_KEY=minio_access_key
MINIO_SECRET_KEY=minio_secret_key
MINIO_ENDPOINT=http://minio:9000 # or http://localhost:9000
# Minio browser redirect URL, suitable for production environment with real domain like https://minio.yourdomain.com
MINIO_BROWSER_REDIRECT_URL=http://localhost:9000
MINIO_API_PORT=9000
MINIO_CONSOLE_PORT=9001
MINIO_SSL_ENABLED=false

# ------------------------------
# Database configuration
# The database uses PostgreSQL
# In this project we have:
# (1) a database serves as a operational database storing AdventureWorks2014 data
# (2) a database serves as a metadata database storing metadata of the data platform, used by services like Airflow, Hive, etc.
# ------------------------------

# Metadata database
META_DB_USER=hive
META_DB_PASSWORD=hive
META_DB_NAME=metastore
META_DB_DRIVER=org.postgresql.Driver
META_DB_URL=jdbc:postgresql://metastore_db:5432/$META_DB_NAME # or jdbc:postgresql://localhost:5432/$META_DB_NAME
META_DB_PORT_EXPOSE=5433

# Operational database
OLTP_DB_USER=adventureworks
OLTP_DB_PASSWORD=adventureworks
OLTP_DB_NAME=adventureworks
OLTP_DB_PORT_EXPOSE=6543
OLTP_DB_HOST=oltp
OLTP_DB_PORT=5432

# ------------------------------
# Hive metastore configuration
# This Hive metastore configuration is used for Hive service
# ------------------------------
HIVE_METASTORE_PORT_EXPOSE=9083
HIVE_METASTORE_URI=thrift://hive-metastore:9083

# ------------------------------
# Spark configuration
# This Spark configuration is used for Spark cluster
# ------------------------------

# Spark master configuration
SPARK_MASTER_PORT_EXPOSE=7077
SPARK_MASTER_WEBUI_PORT_EXPOSE=8081

# Spark worker configuration
SPARK_WORKER_CORES=1
SPARK_WORKER_MEMORY=1g
SPARK_MASTER_URL=spark://spark-master:7077
SPARK_WORKER_IP_RANGE=8091-8100

# ------------------------------
# Trino configuration
# This Trino configuration is used for Trino cluster
# ------------------------------
TRINO_PORT_EXPOSE=8090

# ------------------------------
# Nginx configuration
# This Nginx configuration is used for reverse proxy
# ------------------------------
NGINX_SERVER_NAME=domain.name
NGINX_HTTPS_ENABLED=false
# HTTP port
NGINX_PORT=80
# SSL settings are only applied when NGINX_HTTPS_ENABLED is true
NGINX_SSL_PORT=443
# if NGINX_HTTPS_ENABLED is true, SSL certificates/keys must be placed in `./nginx/ssl` directory
# and modify the env vars below accordingly
NGINX_SSL_CERT_FILENAME=fullchain.pem
NGINX_SSL_CERT_KEY_FILENAME=privkey.pem
NGINX_SSL_PROTOCOLS="TLSv1.1 TLSv1.2 TLSv1.3"

# Nginx performance tuning
NGINX_WORKER_PROCESSES=auto
NGINX_CLIENT_MAX_BODY_SIZE=15M
NGINX_KEEPALIVE_TIMEOUT=65

# Proxy setting
NGINX_PROXY_READ_TIMEOUT=3600s
NGINX_PROXY_SEND_TIMEOUT=3600s

# Set true to accept requests for /.well-know/acme-challenge/
NGINX_ENABLE_CERTBOT_CHALLENGE=false

# ------------------------------
# Cerbot configuration
# This Cerbot configuration is used for SSL certificate
# ------------------------------
# Email address (required to get certificates from Let's Encrypt)
CERTBOT_EMAIL=mail@domain.name

# Domain name
CERTBOT_DOMAIN=domain.name

# cerbot command options
# i.e: --force-renewal --dry-run --test-cert --debug
CERTBOT_OPTIONS=""

# ------------------------------
# Docker Compose Service Expose Host Port Configurations
# ------------------------------
EXPOSE_NGINX_PORT=80
EXPOSE_NGINX_SSL_PORT=443