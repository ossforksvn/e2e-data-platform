# x-shared-env: &shared-env
#   AWS_ACCESS_KEY: {$AWS_ACCESS_KEY}
#   AWS_SECRET_KEY: {$AWS_SECRET_KEY}
#   AWS_S3_ENDPOINT: {$AWS_S3_ENDPOINT}
#   PG_HOST: {$PG_HOST}
#   PG_PORT: {$PG_PORT}
#   PG_USER: {$PG_USER}
#   PG_PWD: {$PG_PWD}
#   PG_DB: {$PG_DB}

services:
  minio:
    image: minio/minio:RELEASE.2024-09-13T20-26-02Z
    container_name: minio
    hostname: minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY:-minio_access_key}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY:-minio_secret_key}
      # MINIO_BROWSER_REDIRECT_URL: ${MINIO_BROWSER_REDIRECT_URL:-https://minio-console.example.com}
    volumes:
      - ./volumes/minio:/data
    ports:
      - ${MINIO_API_PORT:-9000}:9000
      - ${MINIO_CONSOLE_PORT:-9001}:9001
    command: server /data --console-address ":${MINIO_CONSOLE_PORT:-9001}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MINIO_API_PORT:-9000}/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - data-network

  createbucket:
    hostname: createbucket
    container_name: createbucket
    image: minio/mc:RELEASE.2024-01-13T08-44-48Z
    depends_on:
      - minio
    environment:
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minio_access_key}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minio_secret_key}
      # custom env vars
      MINIO_ENDPOINT: ${MINIO_ENDPOINT:-http://minio:9000}
      DATALAKE_BUCKET: ${DATALAKE_BUCKET:-delta}
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set myminio ${MINIO_ENDPOINT} ${MINIO_ACCESS_KEY} ${MINIO_SECRET_KEY};
      /usr/bin/mc mb myminio/delta;
      "
    # entrypoint: >
    #   /bin/sh -c "
    #   /usr/bin/mc alias set myminio ${MINIO_ENDPOINT} ${MINIO_ACCESS_KEY} ${MINIO_SECRET_KEY};
    #   /usr/bin/mc mb myminio/delta;
    #   /usr/bin/mc anonymous set public myminio/delta;
    #   "
    networks:
      - data-network

  oltp:
    hostname: oltp
    container_name: oltp-adw14
    image: postgres:16-bullseye
    environment:
      POSTGRES_USER: ${OLTP_DB_USER:-adventureworks}
      POSTGRES_PASSWORD: ${OLTP_DB_PASSWORD:-adventureworks}
      POSTGRES_DB: ${OLTP_DB_NAME:-adventureworks}
      # custom env vars
      OLTP_DB_PORT_EXPOSE: ${OLTP_DB_PORT_EXPOSE:-6543}
    ports:
      - ${OLTP_DB_PORT_EXPOSE}:5432
    volumes:
      - ./volumes/oltp:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready"]
      interval: 1s
      timeout: 10s
      retries: 10
    networks:
      - data-network

  trino:
    container_name: trino
    image: "trinodb/trino:455"
    hostname: trino
    user: root
    # restart: on-failure
    environment:
      # custom env vars
      TRINO_PORT_EXPOSE: ${TRINO_PORT_EXPOSE:-8090}
      # oltp catalog
      OLTP_DB_HOST: ${OLTP_DB_HOST:-oltp}
      OLTP_DB_PORT: ${OLTP_DB_PORT:-5432}
      OLTP_DB_NAME: ${OLTP_DB_NAME:-adventureworks}
      OLTP_DB_USER: ${OLTP_DB_USER:-adventureworks}
      OLTP_DB_PASSWORD: ${OLTP_DB_PASSWORD:-adventureworks}
      # delta catalog
      HIVE_METASTORE_URI: ${HIVE_METASTORE_URI:-thrift://hive-metastore:9083}
      MINIO_ENDPOINT: ${MINIO_ENDPOINT:-http://minio:9000}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minio_access_key}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minio_secret_key}
      MINIO_SSL_ENABLED: ${MINIO_SSL_ENABLED:-false}
    volumes:
      - ./trino/etc:/usr/lib/trino/etc:ro
      - ./trino/catalog:/etc/trino/catalog
      - ./trino/templates:/etc/trino-template
      - ./trino/trinoconfigcatalog.sh:/docker-entrypoint-mount.sh
    entrypoint: [ "sh", "-c", "sed 's/\r$$//' /docker-entrypoint-mount.sh > /docker-entrypoint-clean.sh && chmod +x /docker-entrypoint-clean.sh && /docker-entrypoint-clean.sh && /usr/lib/trino/bin/run-trino" ]
    ports:
      - "${TRINO_PORT_EXPOSE}:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 30s
      timeout: 20s
      retries: 3
    depends_on:
      - hive-metastore
    networks:
      - data-network

  # We use PostgreSQL to store Hive metadata about
  # how the datafile are mapped to schemas and tables
  metastore_db:
    container_name: metastore-db
    image: postgres:16-bullseye
    hostname: metastore_db
    environment:
      POSTGRES_USER: ${META_DB_USER:-hive}
      POSTGRES_PASSWORD: ${META_DB_PASSWORD:-hive}
      POSTGRES_DB: ${META_DB_NAME:-metastore}
      # custom env vars
      META_DB_PORT_EXPOSE: ${META_DB_PORT_EXPOSE:-5433}
    ports:
      - "${META_DB_PORT_EXPOSE}:5432"
    volumes:
      - ./volumes/metastore:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready"]
      interval: 1s
      timeout: 10s
      retries: 10
    networks:
      - data-network

  # Expose service to get metadata, which is a repository of metadata about the tables,
  # such as database names, table names, schema and data location of each table
  hive-metastore:
    container_name: hive-metastore
    image: "starburstdata/hive:3.1.2-e.18"
    hostname: hive-metastore
    environment:
      HIVE_METASTORE_DRIVER: ${META_DB_DRIVER:-org.postgresql.Driver}
      HIVE_METASTORE_JDBC_URL: ${META_DB_URL:-jdbc:postgresql://metastore_db:5432/metastore}
      HIVE_METASTORE_USER: ${META_DB_USER:-hive}
      HIVE_METASTORE_PASSWORD: ${META_DB_PASSWORD:-hive}
      HIVE_METASTORE_WAREHOUSE_DIR: ${WAREHOUSE_DIR:-s3://delta} # HDFS config, we don't need it
      HIVE_METASTORE_USERS_IN_ADMIN_ROLE: "admin" # We also don't need it
      S3_ENDPOINT: ${MINIO_ENDPOINT:-http://minio:9000}
      S3_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minio_access_key}
      S3_SECRET_KEY: ${MINIO_SECRET_KEY:-minio_secret_key}
      S3_PATH_STYLE_ACCESS: "true"
      # Below arguments exist for no reasons, but
      # we can not live without it
      REGION: ""
      GOOGLE_CLOUD_KEY_FILE_PATH: ""
      AZURE_ADL_CLIENT_ID: ""
      AZURE_ADL_CREDENTIAL: ""
      AZURE_ADL_REFRESH_URL: ""
      AZURE_ABFS_STORAGE_ACCOUNT: ""
      AZURE_ABFS_ACCESS_KEY: ""
      AZURE_WASB_STORAGE_ACCOUNT: ""
      AZURE_ABFS_OAUTH: ""
      AZURE_ABFS_OAUTH_TOKEN_PROVIDER: ""
      AZURE_ABFS_OAUTH_CLIENT_ID: ""
      AZURE_ABFS_OAUTH_SECRET: ""
      AZURE_ABFS_OAUTH_ENDPOINT: ""
      AZURE_WASB_ACCESS_KEY: ""
      # custom env vars
      HIVE_METASTORE_PORT_EXPOSE: ${HIVE_METASTORE_PORT_EXPOSE:-9083}
    ports:
      - "${HIVE_METASTORE_PORT_EXPOSE}:9083"
    depends_on:
      - metastore_db
    networks:
      - data-network

  # hive-metastore:
  #   hostname: hive-metastore
  #   container_name: hive-metastore
  #   build: .
  #   ports:
  #     - 9083:9083
  #   environment:
  #     SERVICE_NAME: metastore
  #     DB_DRIVER: postgres
  #     HIVE_CUSTOM_CONF_DIR: /opt/hive/conf
  #   volumes:
  #     - ./hive-config:/opt/hive/conf
  #     - ./hadoop-libs/hadoop-aws-3.1.0.jar:/opt/hive/lib/hadoop-aws-3.1.0.jar
  #     - ./hadoop-libs/aws-java-sdk-bundle-1.11.271.jar:/opt/hive/lib/aws-java-sdk-bundle-1.11.271.jar
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   networks:
  #     - wba-network

  # hive-server:
  #   hostname: hive-server
  #   container_name: hive-server
  #   image: apache/hive:3.1.3
  #   ports:
  #     - 10000:10000
  #     - 10002:10002
  #   environment:
  #     SERVICE_NAME: hiveserver2
  #     IS_RESUME: "true"
  #     HIVE_CUSTOM_CONF_DIR: /opt/hive/conf
  #   volumes:
  #     - ./hive-config:/opt/hive/conf
  #     - ./hadoop-libs/hadoop-aws-3.1.0.jar:/opt/hive/lib/hadoop-aws-3.1.0.jar
  #     - ./hadoop-libs/aws-java-sdk-bundle-1.11.271.jar:/opt/hive/lib/aws-java-sdk-bundle-1.11.271.jar
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   networks:
  #     - wba-network

  spark-master:
    hostname: spark-master
    container_name: spark-master
    image: bitnami/spark:3.4.1
    command: bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      SPARK_MODE: master
      # custom env vars
      SPARK_MASTER_PORT_EXPOSE: ${SPARK_MASTER_PORT_EXPOSE:-7077}
      SPARK_MASTER_WEBUI_PORT_EXPOSE: ${SPARK_MASTER_WEBUI_PORT_EXPOSE:-8081}
      # oltp database
      OLTP_DB_HOST: ${OLTP_DB_HOST:-oltp}
      OLTP_DB_PORT: ${OLTP_DB_PORT:-5432}
      OLTP_DB_NAME: ${OLTP_DB_NAME:-adventureworks}
      OLTP_DB_USER: ${OLTP_DB_USER:-adventureworks}
      OLTP_DB_PASSWORD: ${OLTP_DB_PASSWORD:-adventureworks}
    volumes:
      - ./spark/spark-apps:/opt/spark-apps
      - ./spark/spark-config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./spark/spark-config/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
      - ./spark/spark-config/core-site.xml:/opt/bitnami/spark/conf/core-site.xml
    ports:
      - "${SPARK_MASTER_WEBUI_PORT_EXPOSE}:8080"
      - "${SPARK_MASTER_PORT_EXPOSE}:7077"
    depends_on:
      - minio
    networks:
      - data-network

  spark-worker:
    hostname: spark-worker
    # container_name: spark-worker
    image: bitnami/spark:3.4.1
    command: bin/spark-class org.apache.spark.deploy.worker.Worker ${SPARK_MASTER_URL}
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES:-1}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY:-1g}
      SPARK_MASTER_URL: ${SPARK_MASTER_URL:-spark://spark-master:7077}
      # custom env vars
      SPARK_WORKER_IP_RANGE: ${SPARK_WORKER_IP_RANGE:-'8091-8100'}
    ports:
      - ${SPARK_WORKER_IP_RANGE}:8081
    networks:
      - data-network

networks:
  data-network:
    external: true