# """
# Docker compose file for PoC data platform - batch processing
# Author: thanhENC (github.com/thanhENC)
# Contact: Van-An Dinh (linkedin.com/in/van-an-dinh)
# """

services:
  # ==================================================
  # MinIO service: S3 compatible object storage
  # ==================================================
  minio:
    image: minio/minio:RELEASE.2024-09-13T20-26-02Z
    container_name: e2e-data-platform-minio
    hostname: minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY:-minio_access_key}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY:-minio_secret_key}
      # MINIO_BROWSER_REDIRECT_URL: ${MINIO_BROWSER_REDIRECT_URL:-https://minio-console.example.com}
    volumes:
      - ./volumes/minio:/data
    ports:
      - ${MINIO_API_PORT:-9000}:9000
      - ${MINIO_CONSOLE_PORT:-9001}:9001
    command: server /data --console-address ":${MINIO_CONSOLE_PORT:-9001}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MINIO_API_PORT:-9000}/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - data-network

  # Create bucket in MinIO: create delta bucket
  createbucket:
    image: minio/mc:RELEASE.2024-01-13T08-44-48Z
    container_name: e2e-data-platform-createbucket
    hostname: createbucket
    depends_on:
      - minio
    environment:
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minio_access_key}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minio_secret_key}
      # custom env vars
      MINIO_ENDPOINT: ${MINIO_ENDPOINT:-http://minio:9000}
      DATALAKE_BUCKET: ${DATALAKE_BUCKET:-delta}
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set myminio ${MINIO_ENDPOINT} ${MINIO_ACCESS_KEY} ${MINIO_SECRET_KEY};
      /usr/bin/mc mb --quiet myminio/delta || true;
      /usr/bin/mc mb --quiet myminio/iceberg || true;
      "
    networks:
      - data-network

  # ==================================================
  # OLTP database service: PostgreSQL AdventureWorks database
  # ==================================================
  oltp:
    image: postgres:16-bullseye
    container_name: e2e-data-platform-oltp-adw14
    hostname: oltp
    environment:
      POSTGRES_USER: ${OLTP_DB_USER:-adventureworks}
      POSTGRES_PASSWORD: ${OLTP_DB_PASSWORD:-adventureworks}
      POSTGRES_DB: ${OLTP_DB_NAME:-adventureworks}
      # custom env vars
      OLTP_DB_PORT_EXPOSE: ${OLTP_DB_PORT_EXPOSE:-6543}
    ports:
      - ${OLTP_DB_PORT_EXPOSE}:5432
    volumes:
      - ./volumes/oltp:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready"]
      interval: 1s
      timeout: 10s
      retries: 10
    networks:
      - data-network

  # ==================================================
  # Hive metastore service consists of PostgreSQL (metadata) and Hive metastore
  # ==================================================
  # We use PostgreSQL to store Hive metadata about
  # how the datafile are mapped to schemas and tables
  hive_metastore_db:
    image: postgres:16-bullseye
    container_name: e2e-data-platform-hive-metastore-db
    hostname: hive_metastore_db
    environment:
      POSTGRES_USER: ${META_DB_USER:-hive}
      POSTGRES_PASSWORD: ${META_DB_PASSWORD:-hive}
      POSTGRES_DB: ${META_DB_NAME:-metastore}
      # custom env vars
      META_DB_PORT_EXPOSE: ${META_DB_PORT_EXPOSE:-5433}
    ports:
      - "${META_DB_PORT_EXPOSE}:5432"
    volumes:
      - ./volumes/metastore:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready"]
      interval: 1s
      timeout: 10s
      retries: 10
    networks:
      - data-network

  # Expose service to get metadata, which is a repository of metadata about the tables,
  # such as database names, table names, schema and data location of each table
  hive-metastore:
    image: "starburstdata/hive:3.1.2-e.18"
    container_name: e2e-data-platform-hive-metastore
    hostname: hive-metastore
    environment:
      HIVE_METASTORE_DRIVER: ${META_DB_DRIVER:-org.postgresql.Driver}
      HIVE_METASTORE_JDBC_URL: ${META_DB_URL:-jdbc:postgresql://hive_metastore_db:5432/metastore}
      HIVE_METASTORE_USER: ${META_DB_USER:-hive}
      HIVE_METASTORE_PASSWORD: ${META_DB_PASSWORD:-hive}
      HIVE_METASTORE_WAREHOUSE_DIR: ${WAREHOUSE_DIR:-s3://delta}
      HIVE_METASTORE_USERS_IN_ADMIN_ROLE: "admin"
      S3_ENDPOINT: ${MINIO_ENDPOINT:-http://minio:9000}
      S3_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minio_access_key}
      S3_SECRET_KEY: ${MINIO_SECRET_KEY:-minio_secret_key}
      S3_PATH_STYLE_ACCESS: "true"
      REGION: ""
      GOOGLE_CLOUD_KEY_FILE_PATH: ""
      AZURE_ADL_CLIENT_ID: ""
      AZURE_ADL_CREDENTIAL: ""
      AZURE_ADL_REFRESH_URL: ""
      AZURE_ABFS_STORAGE_ACCOUNT: ""
      AZURE_ABFS_ACCESS_KEY: ""
      AZURE_WASB_STORAGE_ACCOUNT: ""
      AZURE_ABFS_OAUTH: ""
      AZURE_ABFS_OAUTH_TOKEN_PROVIDER: ""
      AZURE_ABFS_OAUTH_CLIENT_ID: ""
      AZURE_ABFS_OAUTH_SECRET: ""
      AZURE_ABFS_OAUTH_ENDPOINT: ""
      AZURE_WASB_ACCESS_KEY: ""
      # custom env vars
      HIVE_METASTORE_PORT_EXPOSE: ${HIVE_METASTORE_PORT_EXPOSE:-9083}
    ports:
      - "${HIVE_METASTORE_PORT_EXPOSE}:9083"
    volumes:
      - ./dremio/config/core-site.xml:/opt/dremio/conf/core-site.xml
    depends_on:
      - hive_metastore_db
      - minio
    networks:
      - data-network


  # ================================================== With profiles below ==================================================

  # ==================================================
  # Dremio
  # ==================================================
  dremio:
    profiles: ["dremio"]
    image: dremio/dremio-oss:26.0.0
    container_name: e2e-data-platform-dremio
    hostname: dremio
    restart: unless-stopped
    ports:
      - ${DREMIO_PORT:-9047}:9047   # Web UI / REST API
      - ${DREMIO_JDBC_PORT:-31010}:31010 # JDBC/ODBC port
      - ${DREMIO_ARROW_PORT:-32010}:32010 # ARROW Flight port
      - ${DREMIO_INTERNAL_PORT:-45678}:45678 # Internal communication
    environment:
      - DREMIO_JAVA_SERVER_EXTRA_OPTS=-Dpaths.dist=file:///opt/dremio/data/dist
    volumes:
      - ./dremio/config/core-site.xml:/opt/dremio/conf/core-site.xml
      # - ./volumes/dremio/data/db:/opt/dremio/data/db
      # - ./volumes/dremio/data/logs:/opt/dremio/data/logs
      # - ./volumes/dremio/data/dist:/opt/dremio/data/dist
    networks:
      - data-network

  # ==================================================
  # Trino: distributed SQL query engine for big data
  # Trino cluster service consists of Trino `coordinator` and `worker`
  # ==================================================
  # Trino coordinator service: client-facing service that accepts incoming queries
  trino:
    profiles: [ "trino" ]
    image: "trinodb/trino:455"
    container_name: e2e-data-platform-trino
    hostname: trino
    user: root
    # restart: on-failure
    environment:
      # custom env vars
      TRINO_PORT_EXPOSE: ${TRINO_PORT_EXPOSE:-8090}
      # oltp catalog
      OLTP_DB_HOST: ${OLTP_DB_HOST:-oltp}
      OLTP_DB_PORT: ${OLTP_DB_PORT:-5432}
      OLTP_DB_NAME: ${OLTP_DB_NAME:-adventureworks}
      OLTP_DB_USER: ${OLTP_DB_USER:-adventureworks}
      OLTP_DB_PASSWORD: ${OLTP_DB_PASSWORD:-adventureworks}
      # delta catalog
      HIVE_METASTORE_URI: ${HIVE_METASTORE_URI:-thrift://hive-metastore:9083}
      MINIO_ENDPOINT: ${MINIO_ENDPOINT:-http://minio:9000}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minio_access_key}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minio_secret_key}
      MINIO_SSL_ENABLED: ${MINIO_SSL_ENABLED:-false}
    volumes:
      - ./trino/etc/jvm.config:/etc/trino/jvm.config
      - ./trino/etc/coordinator.config.properties:/etc/trino/config.properties
      - ./trino/etc/coordinator.node.properties:/etc/trino/node.properties
      - ./trino/etc/password-authenticator.properties:/etc/trino/password-authenticator.properties
      - ./trino/etc/password.db:/usr/lib/trino/pwd/password.db
      - ./trino/etc/access-control.properties:/etc/trino/access-control.properties
      - ./trino/etc/rules.json:/etc/rules.json
      - ./trino/catalog:/etc/trino/catalog
      - ./trino/templates:/etc/trino-template
      - ./trino/trinoconfigcatalog.sh:/docker-entrypoint-mount.sh
    entrypoint: [ "sh", "-c", "sed 's/\r$$//' /docker-entrypoint-mount.sh > /docker-entrypoint-clean.sh && chmod +x /docker-entrypoint-clean.sh && /docker-entrypoint-clean.sh && /usr/lib/trino/bin/run-trino" ]
    ports:
      - "${TRINO_PORT_EXPOSE}:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 30s
      timeout: 20s
      retries: 3
    depends_on:
      - hive-metastore
    networks:
      - data-network

  # Trino worker service: worker nodes that execute the query
  trino-worker:
    profiles: [ "trino" ]
    image: "trinodb/trino:455"
    container_name: e2e-data-platform-trino-worker
    volumes:
      - ./trino/etc/jvm.config:/etc/trino/jvm.config
      - ./trino/etc/worker.config.properties:/etc/trino/config.properties
      - ./trino/etc/worker.node.properties:/etc/trino/node.properties
      - ./trino/catalog:/etc/trino/catalog
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 30s
      timeout: 20s
      retries: 3
    depends_on:
      - trino
    networks:
      - data-network

  # ==================================================
  # Spark cluster service consists of Spark master and worker
  # use `docker-compose --profile spark up` to start the Spark cluster.
  # ==================================================
  spark-master:
    profiles: [ "spark" ]
    image: bitnami/spark:3.4.1
    container_name: e2e-data-platform-spark-master
    hostname: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      SPARK_MODE: master
      # custom env vars
      SPARK_MASTER_WEBUI_PORT_EXPOSE: ${SPARK_MASTER_WEBUI_PORT_EXPOSE:-8080}
      SPARK_MASTER_PORT_EXPOSE: ${SPARK_MASTER_PORT_EXPOSE:-7077}
      # oltp database
      OLTP_DB_HOST: ${OLTP_DB_HOST:-oltp}
      OLTP_DB_PORT: ${OLTP_DB_PORT:-5432}
      OLTP_DB_NAME: ${OLTP_DB_NAME:-adventureworks}
      OLTP_DB_USER: ${OLTP_DB_USER:-adventureworks}
      OLTP_DB_PASSWORD: ${OLTP_DB_PASSWORD:-adventureworks}
    volumes:
      - ./spark/spark-config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./spark/spark-config/core-site.xml:/opt/bitnami/spark/conf/core-site.xml
      - ./spark/spark-config/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
      - ./spark/spark-apps:/opt/spark-apps
    ports:
      - "${SPARK_MASTER_WEBUI_PORT_EXPOSE}:8080"
      - "${SPARK_MASTER_PORT_EXPOSE}:7077"
    depends_on:
      - minio
    networks:
      - data-network

  spark-worker:
    profiles: [ "spark" ]
    image: bitnami/spark:3.4.1
    container_name: e2e-data-platform-spark-worker
    hostname: spark-worker
    command: bin/spark-class org.apache.spark.deploy.worker.Worker ${SPARK_MASTER_URL}
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES:-1}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY:-1g}
      SPARK_MASTER_URL: ${SPARK_MASTER_URL:-spark://spark-master:7077}
      # custom env vars
      SPARK_WORKER_IP_RANGE: ${SPARK_WORKER_IP_RANGE:-'8091-8100'}
    ports:
      - ${SPARK_WORKER_IP_RANGE}:8081
    networks:
      - data-network

  spark-jupyter:
    profiles: [ "spark-jupyter" ]
    build:
      dockerfile: ./spark/Dockerfile-spark3.4
    image: 1ambda/lakehouse:spark-3.4
    container_name: e2e-data-platform-spark-jupyter
    hostname: localhost
    entrypoint: |
      /bin/bash -c "
      jupyter lab --notebook-dir=/opt/notebook --ip='*' --NotebookApp.token='' --NotebookApp.password='' --port=8888 --no-browser --allow-root
      "
    ports:
      - ${SPARK_DRIVER_PORT:-4040}:4040
      - ${SPARK_JUPYTER_PORT:-8900}:8888
    depends_on:
      - minio
      - hive-metastore
    environment:
      # S3_ENDPOINT: ${S3_ENDPOINT:-http://minio:9000}
      # AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-minio_access_key}
      # AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-minio_secret_key}
      # AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-}
      # AWS_REGION: ${AWS_REGION:-}
      # S3_PATH_STYLE_ACCESS: ${S3_PATH_STYLE_ACCESS:-true}

      # oltp database
      OLTP_DB_HOST: ${OLTP_DB_HOST:-oltp}
      OLTP_DB_PORT: ${OLTP_DB_PORT:-5432}
      OLTP_DB_NAME: ${OLTP_DB_NAME:-adventureworks}
      OLTP_DB_USER: ${OLTP_DB_USER:-adventureworks}
      OLTP_DB_PASSWORD: ${OLTP_DB_PASSWORD:-adventureworks}
    volumes:
      - ./spark/spark-config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./spark/spark-config/core-site.xml:/opt/spark/conf/core-site.xml
      - ./spark/spark-config/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./jupyter/jupyter_server_config.py:/root/.jupyter/jupyter_server_config.py
      - ./jupyter/themes.jupyterlab-settings:/root/.jupyter/lab/user-settings/@jupyterlab/apputils-extension/themes.jupyterlab-settings
      - ../notebook:/opt/notebook
    networks:
      - data-network

  spark-iceberg-jupyter:
    profiles: [ "spark-iceberg-jupyter" ]
    build:
      dockerfile: ./spark/Dockerfile-spark3.4
    image: 1ambda/lakehouse:spark-3.4
    container_name: e2e-data-platform-spark-iceberg-jupyter
    hostname: localhost
    entrypoint: |
      /bin/bash -c "
      jupyter lab --notebook-dir=/opt/notebook --ip='*' --NotebookApp.token='' --NotebookApp.password='' --port=8888 --no-browser --allow-root
      "
    ports:
      - ${SPARK_ICEBERG_DRIVER_PORT:-4041}:4040
      - ${SPARK_ICEBERG_JUPYTER_PORT:-8901}:8888
    depends_on:
      - minio
      - hive-metastore
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-minio_access_key}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-minio_secret_key}
      AWS_REGION: ${AWS_REGION:-us-east-1}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-us-east-1}
      S3_ENDPOINT: ${S3_ENDPOINT:-http://minio:9000}
      S3_PATH_STYLE_ACCESS: ${S3_PATH_STYLE_ACCESS:-true}
    volumes:
      - ./spark/spark-defaults-iceberg.conf:/opt/spark/conf/spark-defaults.conf
      - ./jupyter/jupyter_server_config.py:/root/.jupyter/jupyter_server_config.py
      - ./jupyter/themes.jupyterlab-settings:/root/.jupyter/lab/user-settings/@jupyterlab/apputils-extension/themes.jupyterlab-settings
      - ../notebook:/opt/notebook
    networks:
      - data-network

  ####################################################################################################
  # Flink
  ####################################################################################################
  flink-jobmanager:
    profiles: [ "flink" ]
    build:
      dockerfile: ./flink/Dockerfile-flink1.16
    image: 1ambda/lakehouse:flink-1.16
    container_name: lakehouse-playground-flink-jobmanager
    hostname: flink-jobmanager
    entrypoint: |
      /bin/bash -c "
      export HADOOP_CLASSPATH=`/opt/hadoop/bin/hadoop classpath`;
      /docker-entrypoint.sh jobmanager;
      "
    working_dir: /opt/flink
    ports:
      - "8082:8081"
      - "6123:6123"
    environment:
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=minio123
      - AWS_REGION=us-east-1
      - AWS_DEFAULT_REGION=us-east-1
      - S3_ENDPOINT=http://minio:9000
      - S3_PATH_STYLE_ACCESS=true
      - |
        FLINK_PROPERTIES=
        fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
        fs.s3a.access.key: minio
        fs.s3a.secret.key: minio123
        fs.s3a.endpoint: http://minio:9000
        fs.s3a.path.style.access: true
        jobmanager.rpc.address: flink-jobmanager
        state.backend: rocksdb
        state.backend.incremental: true
        state.checkpoints.dir: s3a://datalake/flink/cluster-common/checkpoints/
        state.savepoints.dir: s3a://datalake/flink/cluster-common/savepoints/
    volumes:
      - ./flink/hadoop-core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./flink/hadoop-hive-site.xml:/opt/flink/conf/hive-site.xml
      - ./flink/flink-sql-hudi.sh:/opt/flink-client/flink-sql-hudi
      - ./flink/flink-init-hudi.sql:/opt/flink-client/flink-init-hudi.sql
      - ./flink/flink-sql-iceberg.sh:/opt/flink-client/flink-sql-iceberg
      - ./flink/flink-init-iceberg.sql:/opt/flink-client/flink-init-iceberg.sql

  flink-taskmanager:
    profiles: [ "flink" ]
    build:
      dockerfile: ./flink/Dockerfile-flink1.16
    image: 1ambda/lakehouse:flink-1.16
    container_name: lakehouse-playground-flink-taskmanager
    hostname: flink-taskmanager
    entrypoint: |
      /bin/bash -c "
      export HADOOP_CLASSPATH=`/opt/hadoop//bin/hadoop classpath`;
      /docker-entrypoint.sh taskmanager;
      "
    working_dir: /opt/flink
    environment:
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=minio123
      - AWS_REGION=us-east-1
      - AWS_DEFAULT_REGION=us-east-1
      - S3_ENDPOINT=http://minio:9000
      - S3_PATH_STYLE_ACCESS=true
      - |
        FLINK_PROPERTIES=
        fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
        fs.s3a.access.key: minio
        fs.s3a.secret.key: minio123
        fs.s3a.endpoint: http://minio:9000
        fs.s3a.path.style.access: true
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 8
        parallelism.default: 1
        state.backend: rocksdb
        state.backend.incremental: true
        state.checkpoints.dir: s3a://datalake/flink/cluster-common/checkpoints/
        state.savepoints.dir: s3a://datalake/flink/cluster-common/savepoints/
    volumes:
      - ./flink/hadoop-core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./flink/hadoop-hive-site.xml:/opt/flink/conf/hive-site.xml
    depends_on:
      - flink-jobmanager


  # ==================================================
  # Lightdash service: BI tool for data exploration
  # use `docker-compose --profile lightdash up` to start the Lightdash service.
  # ==================================================
  lightdash:
    profiles: [ "lightdash" ]
    platform: linux/amd64
    image: lightdash/lightdash:latest
    container_name: e2e-data-platform-lightdash
    depends_on:
      - lightdash-db
    environment:
      - PGHOST=${LD_PGHOST:-"lightdash-db"}
      - PGPORT=${LD_PGPORT:-5432}
      - PGUSER=${LD_PGUSER:-lightdash}
      - PGPASSWORD=${LD_PGPASSWORD:-lightdash}
      - PGDATABASE=${LD_PGDATABASE:-lightdash}
      - SECURE_COOKIES=${SECURE_COOKIES:-false}
      - TRUST_PROXY=${TRUST_PROXY:-false}
      - LIGHTDASH_SECRET=${LIGHTDASH_SECRET}
      - PORT=8080
      - LIGHTDASH_LOG_LEVEL=${LIGHTDASH_LOG_LEVEL}
      - LIGHTDASH_INSTALL_ID=${LIGHTDASH_INSTALL_ID}
      - LIGHTDASH_INSTALL_TYPE=${LIGHTDASH_INSTALL_TYPE:-docker_image}
      - AUTH_DISABLE_PASSWORD_AUTHENTICATION=${AUTH_DISABLE_PASSWORD_AUTHENTICATION}
      - AUTH_ENABLE_GROUP_SYNC=${AUTH_ENABLE_GROUP_SYNC}
      - AUTH_GOOGLE_ENABLED=${AUTH_GOOGLE_ENABLED}
      - AUTH_GOOGLE_OAUTH2_CLIENT_ID=${AUTH_GOOGLE_OAUTH2_CLIENT_ID}
      - AUTH_GOOGLE_OAUTH2_CLIENT_SECRET=${AUTH_GOOGLE_OAUTH2_CLIENT_SECRET}
      - SITE_URL=${SITE_URL}
      - EMAIL_SMTP_HOST=${EMAIL_SMTP_HOST}
      - EMAIL_SMTP_PORT=${EMAIL_SMTP_PORT}
      - EMAIL_SMTP_SECURE=${EMAIL_SMTP_SECURE}
      - EMAIL_SMTP_USER=${EMAIL_SMTP_USER}
      - EMAIL_SMTP_PASSWORD=${EMAIL_SMTP_PASSWORD}
      - EMAIL_SMTP_ALLOW_INVALID_CERT=${EMAIL_SMTP_ALLOW_INVALID_CERT}
      - EMAIL_SMTP_SENDER_NAME=${EMAIL_SMTP_SENDER_NAME}
      - EMAIL_SMTP_SENDER_EMAIL=${EMAIL_SMTP_SENDER_EMAIL}
      - ALLOW_MULTIPLE_ORGS=${ALLOW_MULTIPLE_ORGS:-false}
      - LIGHTDASH_QUERY_MAX_LIMIT=${LIGHTDASH_QUERY_MAX_LIMIT}
      - LIGHTDASH_MAX_PAYLOAD=${LIGHTDASH_MAX_PAYLOAD:-5mb}
      - HEADLESS_BROWSER_HOST=headless-browser
      - HEADLESS_BROWSER_PORT=3000
      - RUDDERSTACK_WRITE_KEY=${RUDDERSTACK_WRITE_KEY}
      - SCHEDULER_ENABLED=true
      - GROUPS_ENABLED=${GROUPS_ENABLED:-false}
      - SLACK_CLIENT_ID=${SLACK_CLIENT_ID}
      - SLACK_CLIENT_SECRET=${SLACK_CLIENT_SECRET}
      - SLACK_SIGNING_SECRET=${SLACK_SIGNING_SECRET}
      - SLACK_STATE_SECRET=${SLACK_STATE_SECRET}
    volumes:
      - ${DBT_PROJECT_DIR}:/usr/app/dbt
    ports:
      - ${LD_PORT_EXPOSE:-8080}:${PORT:-8080}
    networks:
      - data-network

  lightdash-db:
    profiles: [ "lightdash" ]
    image: postgres:15.4
    container_name: e2e-data-platform-lightdash-db
    restart: unless-stopped
    environment:
      POSTGRES_PASSWORD: ${LD_PGPASSWORD:-lightdash}
      POSTGRES_USER: ${LD_PGUSER:-lightdash}
      POSTGRES_DB: ${LD_PGDATABASE:-lightdash}
    volumes:
      - ./volumes/lightdash-db:/var/lib/postgresql/data
    ports:
      - 54321:5432
    networks:
      - data-network

  headless-browser:
    profiles: [ "lightdash" ]
    image: browserless/chrome
    container_name: e2e-data-platform-headless-browser
    restart: unless-stopped
    ports:
      - "3001:3000"
    networks:
      - data-network

  # ==================================================
  # DBT docs service: documentation for dbt models
  # ==================================================
  dbt:
    profiles: [ "dbt" ]
    build:
      context: ../dbt
      dockerfile: Dockerfile
    image: theweekdays/dbt:0.0.1
    container_name: e2e-data-platform-dbt
    hostname: dbt
    # entrypoint: >
    #   sh -c "dbt docs generate --no-compile"
    volumes:
      - ./volumes/dbt:/usr/dbt/target
      - ../dbt:/usr/src/dbt # dbt project source code directory
    networks:
      - data-network

  # ==================================================
  # Certbot service: Let's Encrypt service for SSL certificates
  # use `docker-compose --profile certbot up` to start the certbot service.
  # ==================================================
  certbot:
    profiles: [ "certbot" ]
    image: certbot/certbot
    container_name: e2e-data-platform-certbot
    volumes:
      - ./volumes/certbot/conf:/etc/letsencrypt
      - ./volumes/certbot/www:/var/www/html
      - ./volumes/certbot/logs:/var/log/letsencrypt
      - ./volumes/certbot/conf/live:/etc/letsencrypt/live
      - ./certbot/update-cert.template.txt:/update-cert.template.txt
      - ./certbot/docker-entrypoint.sh:/docker-entrypoint.sh
    environment:
      - CERTBOT_EMAIL=${CERTBOT_EMAIL}
      - CERTBOT_DOMAIN=${CERTBOT_DOMAIN}
      - CERTBOT_OPTIONS=${CERTBOT_OPTIONS:-}
    entrypoint: [ "/docker-entrypoint.sh" ]
    command: [ "tail", "-f", "/dev/null" ]

  # ==================================================
  # Nginx service: reverse proxy service
  # used for reverse proxying the Trino service and other web service.
  # ==================================================
  nginx:
    profiles: [ "trino" ]
    image: nginx:latest
    container_name: e2e-data-platform-nginx
    restart: unless-stopped
    volumes:
      - ./nginx/nginx.conf.template:/etc/nginx/nginx.conf.template
      - ./nginx/proxy.conf.template:/etc/nginx/proxy.conf.template
      - ./nginx/https.conf.template:/etc/nginx/https.conf.template
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./nginx/docker-entrypoint.sh:/docker-entrypoint-mount.sh
      - ./nginx/ssl:/etc/ssl # cert dir (legacy)
      - ./volumes/certbot/conf/live:/etc/letsencrypt/live # cert dir (with certbot container)
      - ./volumes/certbot/conf:/etc/letsencrypt
      - ./volumes/certbot/www:/var/www/html
      - ./volumes/dbt:/var/www/dbt
    entrypoint: [ "sh", "-c", "cp /docker-entrypoint-mount.sh /docker-entrypoint.sh && sed -i 's/\r$$//' /docker-entrypoint.sh && chmod +x /docker-entrypoint.sh && /docker-entrypoint.sh" ]
    environment:
      NGINX_SERVER_NAME: ${NGINX_SERVER_NAME:-_}
      NGINX_HTTPS_ENABLED: ${NGINX_HTTPS_ENABLED:-false}
      NGINX_SSL_PORT: ${NGINX_SSL_PORT:-443}
      NGINX_PORT: ${NGINX_PORT:-80}
      # You're required to add your own SSL certificates/keys to the `./nginx/ssl` directory
      # and modify the env vars below in .env if HTTPS_ENABLED is true.
      NGINX_SSL_CERT_FILENAME: ${NGINX_SSL_CERT_FILENAME:-fullchain.pem}
      NGINX_SSL_CERT_KEY_FILENAME: ${NGINX_SSL_CERT_KEY_FILENAME:-privkey.pem}
      NGINX_SSL_PROTOCOLS: ${NGINX_SSL_PROTOCOLS:-TLSv1.1 TLSv1.2 TLSv1.3}
      NGINX_WORKER_PROCESSES: ${NGINX_WORKER_PROCESSES:-auto}
      NGINX_CLIENT_MAX_BODY_SIZE: ${NGINX_CLIENT_MAX_BODY_SIZE:-15M}
      NGINX_KEEPALIVE_TIMEOUT: ${NGINX_KEEPALIVE_TIMEOUT:-65}
      NGINX_PROXY_READ_TIMEOUT: ${NGINX_PROXY_READ_TIMEOUT:-3600s}
      NGINX_PROXY_SEND_TIMEOUT: ${NGINX_PROXY_SEND_TIMEOUT:-3600s}
      NGINX_ENABLE_CERTBOT_CHALLENGE: ${NGINX_ENABLE_CERTBOT_CHALLENGE:-false}
      CERTBOT_DOMAIN: ${CERTBOT_DOMAIN:-}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${NGINX_PORT:-80}"]
      interval: 30s
      timeout: 20s
      retries: 3
    depends_on:
      trino:
        condition: service_healthy
    ports:
      - "${EXPOSE_NGINX_PORT:-80}:${NGINX_PORT:-80}"
      - "${EXPOSE_NGINX_SSL_PORT:-443}:${NGINX_SSL_PORT:-443}"
    networks:
      - data-network

# ==================================================
# Networks configuration
# data-network: external network for data platform services. This network must be created before running the docker-compose file.
# use `docker network create --driver bridge data-network` to create the network.
# ==================================================
networks:
  data-network:
    external: true