# """
# Docker compose file for PoC data platform - batch processing
# Author: thanhENC (github.com/thanhENC)
# Contact: Van-An Dinh (linkedin.com/in/van-an-dinh)
# """

services:
  # ==================================================
  # MinIO service: S3 compatible object storage
  # ==================================================
  minio:
    image: minio/minio:RELEASE.2024-09-13T20-26-02Z
    container_name: minio
    hostname: minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY:-minio_access_key}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY:-minio_secret_key}
      # MINIO_BROWSER_REDIRECT_URL: ${MINIO_BROWSER_REDIRECT_URL:-https://minio-console.example.com}
    volumes:
      - ./volumes/minio:/data
    ports:
      - ${MINIO_API_PORT:-9000}:9000
      - ${MINIO_CONSOLE_PORT:-9001}:9001
    command: server /data --console-address ":${MINIO_CONSOLE_PORT:-9001}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MINIO_API_PORT:-9000}/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - data-network

  # Create bucket in MinIO: create delta bucket
  createbucket:
    hostname: createbucket
    container_name: createbucket
    image: minio/mc:RELEASE.2024-01-13T08-44-48Z
    depends_on:
      - minio
    environment:
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minio_access_key}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minio_secret_key}
      # custom env vars
      MINIO_ENDPOINT: ${MINIO_ENDPOINT:-http://minio:9000}
      DATALAKE_BUCKET: ${DATALAKE_BUCKET:-delta}
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set myminio ${MINIO_ENDPOINT} ${MINIO_ACCESS_KEY} ${MINIO_SECRET_KEY};
      /usr/bin/mc mb myminio/delta;
      "
    networks:
      - data-network

  # ==================================================
  # OLTP database service: PostgreSQL AdventureWorks database
  # ==================================================
  oltp:
    hostname: oltp
    container_name: oltp-adw14
    image: postgres:16-bullseye
    environment:
      POSTGRES_USER: ${OLTP_DB_USER:-adventureworks}
      POSTGRES_PASSWORD: ${OLTP_DB_PASSWORD:-adventureworks}
      POSTGRES_DB: ${OLTP_DB_NAME:-adventureworks}
      # custom env vars
      OLTP_DB_PORT_EXPOSE: ${OLTP_DB_PORT_EXPOSE:-6543}
    ports:
      - ${OLTP_DB_PORT_EXPOSE}:5432
    volumes:
      - ./volumes/oltp:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready"]
      interval: 1s
      timeout: 10s
      retries: 10
    networks:
      - data-network

  # ==================================================
  # Hive metastore service consists of PostgreSQL (metadata) and Hive metastore
  # ==================================================
  # We use PostgreSQL to store Hive metadata about
  # how the datafile are mapped to schemas and tables
  metastore_db:
    container_name: metastore-db
    image: postgres:16-bullseye
    hostname: metastore_db
    environment:
      POSTGRES_USER: ${META_DB_USER:-hive}
      POSTGRES_PASSWORD: ${META_DB_PASSWORD:-hive}
      POSTGRES_DB: ${META_DB_NAME:-metastore}
      # custom env vars
      META_DB_PORT_EXPOSE: ${META_DB_PORT_EXPOSE:-5433}
    ports:
      - "${META_DB_PORT_EXPOSE}:5432"
    volumes:
      - ./volumes/metastore:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready"]
      interval: 1s
      timeout: 10s
      retries: 10
    networks:
      - data-network

  # Expose service to get metadata, which is a repository of metadata about the tables,
  # such as database names, table names, schema and data location of each table
  hive-metastore:
    container_name: hive-metastore
    image: "starburstdata/hive:3.1.2-e.18"
    hostname: hive-metastore
    environment:
      HIVE_METASTORE_DRIVER: ${META_DB_DRIVER:-org.postgresql.Driver}
      HIVE_METASTORE_JDBC_URL: ${META_DB_URL:-jdbc:postgresql://metastore_db:5432/metastore}
      HIVE_METASTORE_USER: ${META_DB_USER:-hive}
      HIVE_METASTORE_PASSWORD: ${META_DB_PASSWORD:-hive}
      HIVE_METASTORE_WAREHOUSE_DIR: ${WAREHOUSE_DIR:-s3://delta}
      HIVE_METASTORE_USERS_IN_ADMIN_ROLE: "admin"
      S3_ENDPOINT: ${MINIO_ENDPOINT:-http://minio:9000}
      S3_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minio_access_key}
      S3_SECRET_KEY: ${MINIO_SECRET_KEY:-minio_secret_key}
      S3_PATH_STYLE_ACCESS: "true"
      REGION: ""
      GOOGLE_CLOUD_KEY_FILE_PATH: ""
      AZURE_ADL_CLIENT_ID: ""
      AZURE_ADL_CREDENTIAL: ""
      AZURE_ADL_REFRESH_URL: ""
      AZURE_ABFS_STORAGE_ACCOUNT: ""
      AZURE_ABFS_ACCESS_KEY: ""
      AZURE_WASB_STORAGE_ACCOUNT: ""
      AZURE_ABFS_OAUTH: ""
      AZURE_ABFS_OAUTH_TOKEN_PROVIDER: ""
      AZURE_ABFS_OAUTH_CLIENT_ID: ""
      AZURE_ABFS_OAUTH_SECRET: ""
      AZURE_ABFS_OAUTH_ENDPOINT: ""
      AZURE_WASB_ACCESS_KEY: ""
      # custom env vars
      HIVE_METASTORE_PORT_EXPOSE: ${HIVE_METASTORE_PORT_EXPOSE:-9083}
    ports:
      - "${HIVE_METASTORE_PORT_EXPOSE}:9083"
    depends_on:
      - metastore_db
      - minio
    networks:
      - data-network

  # ==================================================
  # Trino: distributed SQL query engine for big data
  # Trino cluster service consists of Trino `coordinator` and `worker`
  # ==================================================
  # Trino coordinator service: client-facing service that accepts incoming queries
  trino:
    container_name: trino
    image: "trinodb/trino:455"
    hostname: trino
    user: root
    profiles:
      - trino
    # restart: on-failure
    environment:
      # custom env vars
      TRINO_PORT_EXPOSE: ${TRINO_PORT_EXPOSE:-8090}
      # oltp catalog
      OLTP_DB_HOST: ${OLTP_DB_HOST:-oltp}
      OLTP_DB_PORT: ${OLTP_DB_PORT:-5432}
      OLTP_DB_NAME: ${OLTP_DB_NAME:-adventureworks}
      OLTP_DB_USER: ${OLTP_DB_USER:-adventureworks}
      OLTP_DB_PASSWORD: ${OLTP_DB_PASSWORD:-adventureworks}
      # delta catalog
      HIVE_METASTORE_URI: ${HIVE_METASTORE_URI:-thrift://hive-metastore:9083}
      MINIO_ENDPOINT: ${MINIO_ENDPOINT:-http://minio:9000}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minio_access_key}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minio_secret_key}
      MINIO_SSL_ENABLED: ${MINIO_SSL_ENABLED:-false}
    volumes:
      - ./trino/etc/jvm.config:/etc/trino/jvm.config
      - ./trino/etc/coordinator.config.properties:/etc/trino/config.properties
      - ./trino/etc/coordinator.node.properties:/etc/trino/node.properties
      - ./trino/etc/password-authenticator.properties:/etc/trino/password-authenticator.properties
      - ./trino/etc/password.db:/usr/lib/trino/pwd/password.db
      - ./trino/etc/access-control.properties:/etc/trino/access-control.properties
      - ./trino/etc/rules.json:/etc/rules.json
      - ./trino/catalog:/etc/trino/catalog
      - ./trino/templates:/etc/trino-template
      - ./trino/trinoconfigcatalog.sh:/docker-entrypoint-mount.sh
    entrypoint: [ "sh", "-c", "sed 's/\r$$//' /docker-entrypoint-mount.sh > /docker-entrypoint-clean.sh && chmod +x /docker-entrypoint-clean.sh && /docker-entrypoint-clean.sh && /usr/lib/trino/bin/run-trino" ]
    ports:
      - "${TRINO_PORT_EXPOSE}:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 30s
      timeout: 20s
      retries: 3
    depends_on:
      - hive-metastore
    networks:
      - data-network

  # Trino worker service: worker nodes that execute the query
  trino-worker:
    image: "trinodb/trino:455"
    profiles:
      - trino
    volumes:
      - ./trino/etc/jvm.config:/etc/trino/jvm.config
      - ./trino/etc/worker.config.properties:/etc/trino/config.properties
      - ./trino/etc/worker.node.properties:/etc/trino/node.properties
      - ./trino/catalog:/etc/trino/catalog
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 30s
      timeout: 20s
      retries: 3
    depends_on:
      - trino
    networks:
      - data-network

  # ==================================================
  # Spark cluster service consists of Spark master and worker
  # use `docker-compose --profile spark up` to start the Spark cluster.
  # ==================================================
  spark-master:
    hostname: spark-master
    container_name: spark-master
    image: bitnami/spark:3.4.1
    command: bin/spark-class org.apache.spark.deploy.master.Master
    profiles:
      - spark
    environment:
      SPARK_MODE: master
      # custom env vars
      SPARK_MASTER_PORT_EXPOSE: ${SPARK_MASTER_PORT_EXPOSE:-7077}
      SPARK_MASTER_WEBUI_PORT_EXPOSE: ${SPARK_MASTER_WEBUI_PORT_EXPOSE:-8081}
      # oltp database
      OLTP_DB_HOST: ${OLTP_DB_HOST:-oltp}
      OLTP_DB_PORT: ${OLTP_DB_PORT:-5432}
      OLTP_DB_NAME: ${OLTP_DB_NAME:-adventureworks}
      OLTP_DB_USER: ${OLTP_DB_USER:-adventureworks}
      OLTP_DB_PASSWORD: ${OLTP_DB_PASSWORD:-adventureworks}
    volumes:
      - ./spark/spark-apps:/opt/spark-apps
      - ./spark/spark-config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./spark/spark-config/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
      - ./spark/spark-config/core-site.xml:/opt/bitnami/spark/conf/core-site.xml
    ports:
      - "${SPARK_MASTER_WEBUI_PORT_EXPOSE}:8080"
      - "${SPARK_MASTER_PORT_EXPOSE}:7077"
    depends_on:
      - minio
    networks:
      - data-network

  spark-worker:
    hostname: spark-worker
    image: bitnami/spark:3.4.1
    command: bin/spark-class org.apache.spark.deploy.worker.Worker ${SPARK_MASTER_URL}
    profiles:
      - spark
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES:-1}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY:-1g}
      SPARK_MASTER_URL: ${SPARK_MASTER_URL:-spark://spark-master:7077}
      # custom env vars
      SPARK_WORKER_IP_RANGE: ${SPARK_WORKER_IP_RANGE:-'8091-8100'}
    ports:
      - ${SPARK_WORKER_IP_RANGE}:8081
    networks:
      - data-network

  # ==================================================
  # Lightdash service: BI tool for data exploration
  # use `docker-compose --profile lightdash up` to start the Lightdash service.
  # ==================================================
  lightdash:
    container_name: lightdash
    platform: linux/amd64
    image: lightdash/lightdash:latest
    depends_on:
      - lightdash-db
    environment:
      - PGHOST=${LD_PGHOST:-"lightdash-db"}
      - PGPORT=${LD_PGPORT:-5432}
      - PGUSER=${LD_PGUSER:-lightdash}
      - PGPASSWORD=${LD_PGPASSWORD:-lightdash}
      - PGDATABASE=${LD_PGDATABASE:-lightdash}
      - SECURE_COOKIES=${SECURE_COOKIES:-false}
      - TRUST_PROXY=${TRUST_PROXY:-false}
      - LIGHTDASH_SECRET=${LIGHTDASH_SECRET}
      - PORT=8080
      - LIGHTDASH_LOG_LEVEL=${LIGHTDASH_LOG_LEVEL}
      - LIGHTDASH_INSTALL_ID=${LIGHTDASH_INSTALL_ID}
      - LIGHTDASH_INSTALL_TYPE=${LIGHTDASH_INSTALL_TYPE:-docker_image}
      - AUTH_DISABLE_PASSWORD_AUTHENTICATION=${AUTH_DISABLE_PASSWORD_AUTHENTICATION}
      - AUTH_ENABLE_GROUP_SYNC=${AUTH_ENABLE_GROUP_SYNC}
      - AUTH_GOOGLE_ENABLED=${AUTH_GOOGLE_ENABLED}
      - AUTH_GOOGLE_OAUTH2_CLIENT_ID=${AUTH_GOOGLE_OAUTH2_CLIENT_ID}
      - AUTH_GOOGLE_OAUTH2_CLIENT_SECRET=${AUTH_GOOGLE_OAUTH2_CLIENT_SECRET}
      - SITE_URL=${SITE_URL}
      - EMAIL_SMTP_HOST=${EMAIL_SMTP_HOST}
      - EMAIL_SMTP_PORT=${EMAIL_SMTP_PORT}
      - EMAIL_SMTP_SECURE=${EMAIL_SMTP_SECURE}
      - EMAIL_SMTP_USER=${EMAIL_SMTP_USER}
      - EMAIL_SMTP_PASSWORD=${EMAIL_SMTP_PASSWORD}
      - EMAIL_SMTP_ALLOW_INVALID_CERT=${EMAIL_SMTP_ALLOW_INVALID_CERT}
      - EMAIL_SMTP_SENDER_NAME=${EMAIL_SMTP_SENDER_NAME}
      - EMAIL_SMTP_SENDER_EMAIL=${EMAIL_SMTP_SENDER_EMAIL}
      - ALLOW_MULTIPLE_ORGS=${ALLOW_MULTIPLE_ORGS:-false}
      - LIGHTDASH_QUERY_MAX_LIMIT=${LIGHTDASH_QUERY_MAX_LIMIT}
      - LIGHTDASH_MAX_PAYLOAD=${LIGHTDASH_MAX_PAYLOAD:-5mb}
      - HEADLESS_BROWSER_HOST=headless-browser
      - HEADLESS_BROWSER_PORT=3000
      - RUDDERSTACK_WRITE_KEY=${RUDDERSTACK_WRITE_KEY}
      - SCHEDULER_ENABLED=true
      - GROUPS_ENABLED=${GROUPS_ENABLED:-false}
      - SLACK_CLIENT_ID=${SLACK_CLIENT_ID}
      - SLACK_CLIENT_SECRET=${SLACK_CLIENT_SECRET}
      - SLACK_SIGNING_SECRET=${SLACK_SIGNING_SECRET}
      - SLACK_STATE_SECRET=${SLACK_STATE_SECRET}
    volumes:
      - ${DBT_PROJECT_DIR}:/usr/app/dbt
    ports:
      - ${LD_PORT_EXPOSE:-8080}:${PORT:-8080}
    networks:
      - data-network

  lightdash-db:
    container_name: lightdash-db
    image: postgres:15.4
    restart: always
    environment:
      POSTGRES_PASSWORD: ${LD_PGPASSWORD:-lightdash}
      POSTGRES_USER: ${LD_PGUSER:-lightdash}
      POSTGRES_DB: ${LD_PGDATABASE:-lightdash}
    volumes:
      - ./volumes/lightdash-db:/var/lib/postgresql/data
    ports:
      - 54321:5432
    networks:
      - data-network

  headless-browser:
    image: browserless/chrome
    restart: always
    ports:
      - "3001:3000"
    networks:
      - data-network
    
  # ==================================================
  # Certbot service: Let's Encrypt service for SSL certificates
  # use `docker-compose --profile certbot up` to start the certbot service.
  # ==================================================
  certbot:
    image: certbot/certbot
    profiles:
      - certbot
    container_name: certbot
    volumes:
      - ./volumes/certbot/conf:/etc/letsencrypt
      - ./volumes/certbot/www:/var/www/html
      - ./volumes/certbot/logs:/var/log/letsencrypt
      - ./volumes/certbot/conf/live:/etc/letsencrypt/live
      - ./certbot/update-cert.template.txt:/update-cert.template.txt
      - ./certbot/docker-entrypoint.sh:/docker-entrypoint.sh
    environment:
      - CERTBOT_EMAIL=${CERTBOT_EMAIL}
      - CERTBOT_DOMAIN=${CERTBOT_DOMAIN}
      - CERTBOT_OPTIONS=${CERTBOT_OPTIONS:-}
    entrypoint: [ "/docker-entrypoint.sh" ]
    command: [ "tail", "-f", "/dev/null" ]

  # ==================================================
  # Nginx service: reverse proxy service
  # used for reverse proxying the Trino service and other web service.
  # ==================================================
  nginx:
    image: nginx:latest
    restart: always
    container_name: nginx
    profiles:
      - trino
    volumes:
      - ./nginx/nginx.conf.template:/etc/nginx/nginx.conf.template
      - ./nginx/proxy.conf.template:/etc/nginx/proxy.conf.template
      - ./nginx/https.conf.template:/etc/nginx/https.conf.template
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./nginx/docker-entrypoint.sh:/docker-entrypoint-mount.sh
      - ./nginx/ssl:/etc/ssl # cert dir (legacy)
      - ./volumes/certbot/conf/live:/etc/letsencrypt/live # cert dir (with certbot container)
      - ./volumes/certbot/conf:/etc/letsencrypt
      - ./volumes/certbot/www:/var/www/html
    entrypoint: [ "sh", "-c", "cp /docker-entrypoint-mount.sh /docker-entrypoint.sh && sed -i 's/\r$$//' /docker-entrypoint.sh && chmod +x /docker-entrypoint.sh && /docker-entrypoint.sh" ]
    environment:
      NGINX_SERVER_NAME: ${NGINX_SERVER_NAME:-_}
      NGINX_HTTPS_ENABLED: ${NGINX_HTTPS_ENABLED:-false}
      NGINX_SSL_PORT: ${NGINX_SSL_PORT:-443}
      NGINX_PORT: ${NGINX_PORT:-80}
      # You're required to add your own SSL certificates/keys to the `./nginx/ssl` directory
      # and modify the env vars below in .env if HTTPS_ENABLED is true.
      NGINX_SSL_CERT_FILENAME: ${NGINX_SSL_CERT_FILENAME:-fullchain.pem}
      NGINX_SSL_CERT_KEY_FILENAME: ${NGINX_SSL_CERT_KEY_FILENAME:-privkey.pem}
      NGINX_SSL_PROTOCOLS: ${NGINX_SSL_PROTOCOLS:-TLSv1.1 TLSv1.2 TLSv1.3}
      NGINX_WORKER_PROCESSES: ${NGINX_WORKER_PROCESSES:-auto}
      NGINX_CLIENT_MAX_BODY_SIZE: ${NGINX_CLIENT_MAX_BODY_SIZE:-15M}
      NGINX_KEEPALIVE_TIMEOUT: ${NGINX_KEEPALIVE_TIMEOUT:-65}
      NGINX_PROXY_READ_TIMEOUT: ${NGINX_PROXY_READ_TIMEOUT:-3600s}
      NGINX_PROXY_SEND_TIMEOUT: ${NGINX_PROXY_SEND_TIMEOUT:-3600s}
      NGINX_ENABLE_CERTBOT_CHALLENGE: ${NGINX_ENABLE_CERTBOT_CHALLENGE:-false}
      CERTBOT_DOMAIN: ${CERTBOT_DOMAIN:-}
    depends_on:
      - trino
    ports:
      - "${EXPOSE_NGINX_PORT:-80}:${NGINX_PORT:-80}"
      - "${EXPOSE_NGINX_SSL_PORT:-443}:${NGINX_SSL_PORT:-443}"
    networks:
      - data-network

# ==================================================
# Networks configuration
# data-network: external network for data platform services. This network must be created before running the docker-compose file.
# use `docker network create --driver bridge data-network` to create the network.
# ==================================================
networks:
  data-network:
    external: true