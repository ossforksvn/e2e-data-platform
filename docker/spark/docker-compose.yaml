# x-shared-env: &shared-env
#   AWS_ACCESS_KEY: {$AWS_ACCESS_KEY}
#   AWS_SECRET_KEY: {$AWS_SECRET_KEY}
#   AWS_S3_ENDPOINT: {$AWS_S3_ENDPOINT}
#   PG_HOST: {$PG_HOST}
#   PG_PORT: {$PG_PORT}
#   PG_USER: {$PG_USER}
#   PG_PWD: {$PG_PWD}
#   PG_DB: {$PG_DB}

services:
  minio:
    image: quay.io/minio/minio
    container_name: minio-datagang
    hostname: minio
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER:-minio_access_key}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-minio_secret_key}
      # - MINIO_BROWSER_REDIRECT_URL=${MINIO_BROWSER_REDIRECT_URL:-https://minio-console.example.com}
    volumes:
      - ./data/minio:/data
    ports:
      - ${MINIO_API_PORT:-9000}:${MINIO_API_PORT:-9000}
      - ${MINIO_CONSOLE_PORT:-9001}:${MINIO_CONSOLE_PORT:-9001}
    command: server /data --console-address ":${MINIO_CONSOLE_PORT:-9001}"
    networks:
      - data-network

  createbucket:
    hostname: createbucket
    container_name: createbucket
    image: minio/mc:RELEASE.2024-01-13T08-44-48Z
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set myminio http://minio:9000 minio_access_key minio_secret_key;
      /usr/bin/mc mb myminio/delta;
      /usr/bin/mc anonymous set public myminio/delta;
      "
    networks:
      - data-network

  # postgres:
  #   hostname: postgres
  #   container_name: postgres
  #   image: postgres:10-alpine
  #   ports:
  #     - 5433:5432
  #   environment:
  #     POSTGRES_USER: admin
  #     POSTGRES_PASSWORD: admin
  #     POSTGRES_DB: metastore_db
  #   volumes:
  #     - ./data/postgres-data:/var/lib/postgresql/data
  #   healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U admin -d metastore_db"]
  #     interval: 1s
  #     timeout: 10s
  #     retries: 10
  #   networks:
  #     - data-network

  trino:
    ports:
      - "8084:8080"
    container_name: datalake-trino
    image: "trinodb/trino:410"
    hostname: trino
    volumes:
      - ../trino/etc:/usr/lib/trino/etc:ro
      - ../trino/catalog:/etc/trino/catalog
    depends_on:
      - hive-metastore
    networks:
      - data-network

  # We use PostgreSQL to store Hive metadata about
  # how the datafile are mapped to schemas and tables
  metastore_db:
    container_name: datalake-metastore-db
    image: postgres:11
    hostname: metastore_db
    ports:
      - "5433:5432" # Access via Thrift protocol
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
    volumes:
      - ./data/postgres-data:/var/lib/postgresql/data
    networks:
      - data-network

  # Expose service to get metadata, which is a repository of metadata about the tables,
  # such as database names, table names, schema and data location of each table
  hive-metastore:
    container_name: datalake-hive-metastore
    image: "starburstdata/hive:3.1.2-e.18"
    hostname: hive-metastore
    ports:
      - "9083:9083" # Access via Thrift protocol
    environment:
      HIVE_METASTORE_DRIVER: org.postgresql.Driver
      HIVE_METASTORE_JDBC_URL: jdbc:postgresql://metastore_db:5432/metastore
      HIVE_METASTORE_USER: hive
      HIVE_METASTORE_PASSWORD: hive
      HIVE_METASTORE_WAREHOUSE_DIR: s3://delta/ # HDFS config, we don't need it
      HIVE_METASTORE_USERS_IN_ADMIN_ROLE: "admin" # We also don't need it
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: minio_access_key
      S3_SECRET_KEY: minio_secret_key
      S3_PATH_STYLE_ACCESS: "true"
      # Below arguments exist for no reasons, but
      # we can not live without it
      REGION: ""
      GOOGLE_CLOUD_KEY_FILE_PATH: ""
      AZURE_ADL_CLIENT_ID: ""
      AZURE_ADL_CREDENTIAL: ""
      AZURE_ADL_REFRESH_URL: ""
      AZURE_ABFS_STORAGE_ACCOUNT: ""
      AZURE_ABFS_ACCESS_KEY: ""
      AZURE_WASB_STORAGE_ACCOUNT: ""
      AZURE_ABFS_OAUTH: ""
      AZURE_ABFS_OAUTH_TOKEN_PROVIDER: ""
      AZURE_ABFS_OAUTH_CLIENT_ID: ""
      AZURE_ABFS_OAUTH_SECRET: ""
      AZURE_ABFS_OAUTH_ENDPOINT: ""
      AZURE_WASB_ACCESS_KEY: ""
    depends_on:
      - metastore_db
    networks:
      - data-network

  # hive-metastore:
  #   hostname: hive-metastore
  #   container_name: hive-metastore
  #   build: .
  #   ports:
  #     - 9083:9083
  #   environment:
  #     SERVICE_NAME: metastore
  #     DB_DRIVER: postgres
  #     HIVE_CUSTOM_CONF_DIR: /opt/hive/conf
  #   volumes:
  #     - ./hive-config:/opt/hive/conf
  #     - ./hadoop-libs/hadoop-aws-3.1.0.jar:/opt/hive/lib/hadoop-aws-3.1.0.jar
  #     - ./hadoop-libs/aws-java-sdk-bundle-1.11.271.jar:/opt/hive/lib/aws-java-sdk-bundle-1.11.271.jar
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   networks:
  #     - wba-network

  # hive-server:
  #   hostname: hive-server
  #   container_name: hive-server
  #   image: apache/hive:3.1.3
  #   ports:
  #     - 10000:10000
  #     - 10002:10002
  #   environment:
  #     SERVICE_NAME: hiveserver2
  #     IS_RESUME: "true"
  #     HIVE_CUSTOM_CONF_DIR: /opt/hive/conf
  #   volumes:
  #     - ./hive-config:/opt/hive/conf
  #     - ./hadoop-libs/hadoop-aws-3.1.0.jar:/opt/hive/lib/hadoop-aws-3.1.0.jar
  #     - ./hadoop-libs/aws-java-sdk-bundle-1.11.271.jar:/opt/hive/lib/aws-java-sdk-bundle-1.11.271.jar
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   networks:
  #     - wba-network

  spark-master:
    hostname: spark-master
    container_name: spark-master
    image: bitnami/spark:3.4.1
    command: bin/spark-class org.apache.spark.deploy.master.Master
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./spark-config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./spark-config/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
    ports:
      - "8081:8080"
      - "7077:7077"
    depends_on:
      - minio
    networks:
      - data-network

  spark-worker:
    hostname: spark-worker
    # container_name: spark-worker
    image: bitnami/spark:3.4.1
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 2g
      SPARK_MASTER_URL: spark://spark-master:7077
    ports:
      - "8082-8086:8081"
    networks:
      - data-network

  # spark-worker-2:
  #   hostname: spark-worker-2
  #   container_name: spark-worker-2
  #   image: bitnami/spark:3.4.1
  #   command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  #   depends_on:
  #     - spark-master
  #   environment:
  #     SPARK_MODE: worker
  #     SPARK_WORKER_CORES: 1
  #     SPARK_WORKER_MEMORY: 2g
  #     SPARK_MASTER_URL: spark://spark-master:7077
    # networks:
    #   - wba-network

#   kyuubi:
#     hostname: kyuubi
#     container_name: kyuubi
#     image: apache/kyuubi:1.8.0-spark
#     volumes:
#       - ./kyuubi-config/kyuubi-defaults.conf:/opt/kyuubi/conf/kyuubi-defaults.conf
#     ports:
#       - "10009:10009"
#       - "10099:10099"
#     depends_on:
#       - spark-master
#     networks:
#       - wba-network

networks:
  data-network:
    external: true